{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AND+XORperceptrons.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnvjsnwITNQE8YfTVR5jRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepSmeag/ML_Colab/blob/main/AND%2BXORperceptrons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSD-ubxddEbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37405525-ada2-4156-989e-b6f846eb1f6f"
      },
      "source": [
        "!git clone https://github.com/DeepSmeag/ML_Colab"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML_Colab'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 47 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTnxk8iadYti"
      },
      "source": [
        "First we load the project so we get access to the files. Then we read the files of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdoKlQNwdYRj"
      },
      "source": [
        "def read_file(file_path):\n",
        "  dataset=[]\n",
        "  with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      parts = line.split(',')\n",
        "      x0,x1,y1,y2 = float(parts[0]), float(parts[1]), float(parts[2]), float(parts[3])\n",
        "      dataset.append((x0,x1,y1,y2))\n",
        "    return dataset\n",
        "\n",
        "dataset = read_file('ML_Colab/datasetAND+XOR.csv')\n",
        "\"\"\"it could also work with numpy and/or pandas to read the whole csv file at the\n",
        "same time, probably more efficient too\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBz_LId6fAam"
      },
      "source": [
        "We now have our dataset ready for use, next let's go and make the vectors for use with our network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF8Oo0fsfMsz"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YndtPJIvjWle"
      },
      "source": [
        "#first layer is input, meaning we take the first 2 columns of the dataset\n",
        "num_trains = len(dataset)\n",
        "X = np.ones((num_trains,1)) \n",
        "#now we add the bias to every training instance\n",
        "X = np.append(X, np.array(dataset)[:,0:2], axis=1)\n",
        "# now we take the outputs\n",
        "Y = np.array(dataset)[:,2:4]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNCs5cTggxQZ",
        "outputId": "3abfd099-2299-4ec4-e3cf-aed25e94335e"
      },
      "source": [
        "#now we create Numpy arrays so that we have our weights\n",
        "#the first layer\n",
        "W1 = np.random.uniform(-1,1,(3,3))\n",
        "#the second\n",
        "W2 = np.random.uniform(-1,1,(4,2))\n",
        "print(\"W1:\\n\",W1,\"\\n\")\n",
        "print(\"W2:\\n\",W2,\"\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:\n",
            " [[ 0.30516658  0.69209501 -0.54270486]\n",
            " [ 0.34941804  0.19615041  0.41225399]\n",
            " [ 0.12901429 -0.12993748  0.80580553]] \n",
            "\n",
            "W2:\n",
            " [[-0.55011425 -0.20962796]\n",
            " [ 0.4405438   0.42368551]\n",
            " [-0.45298555 -0.50613278]\n",
            " [ 0.69285179  0.96539352]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTXmd0W6n3sY"
      },
      "source": [
        "#Now we're getting to the fun stuff\n",
        "#First, defining the necessary functions\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "#Forward Propagation\n",
        "def forward_prop():\n",
        "  #calculating the steps\n",
        "  z2 = np.matmul(X, W1)\n",
        "  a2 = sigmoid(z2)\n",
        "  a2 = np.insert(a2, 0, np.ones((num_trains,)), axis = 1)\n",
        "  z3 = np.matmul(a2, W2)\n",
        "  y_hat = sigmoid(z3)\n",
        "  return z2, a2, z3, y_hat\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCjkG9R3u74D"
      },
      "source": [
        "#Ok, so that was forward propagation\n",
        "#Now, the NN should also be able to learn, so we have to implement Backprop\n",
        "#Since we're guessing at best at the moment as to the correct outputs\n",
        "#We have to define several functions for Backprop to work\n",
        "#These include the partial derivatives with which we'll update the weights matrices W1 and W2\n",
        "#And also the backprop itself which will go over epochs or whatever we may call it\n",
        "\n",
        "# matrices sizes\n",
        "# X = num_trains x 3\n",
        "# W1 = 3 x 3\n",
        "# Z2 = num_trains x 3\n",
        "# A2 = num_trains\n",
        "# W2 = 4 x 2\n",
        "# Y = num_trains x 2\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid_gradient(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "def derivative():\n",
        "  return \n",
        "\n",
        "def backprop(epochs, learning_rate):\n",
        "  for i in range(epochs):\n",
        "    z2, a2, z3, y_hat = forward_prop()\n",
        "\n",
        "backprop(1, 0.01)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNi9VDPNvOb6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}