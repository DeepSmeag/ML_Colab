{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AND+XORperceptrons.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMidPFy5dMe0aKWQKwtFXcn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepSmeag/ML_Colab/blob/main/AND%2BXORperceptrons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSD-ubxddEbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37405525-ada2-4156-989e-b6f846eb1f6f"
      },
      "source": [
        "!git clone https://github.com/DeepSmeag/ML_Colab"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML_Colab'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 47 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTnxk8iadYti"
      },
      "source": [
        "First we load the project so we get access to the files. Then we read the files of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdoKlQNwdYRj"
      },
      "source": [
        "def read_file(file_path):\n",
        "  dataset=[]\n",
        "  with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      parts = line.split(',')\n",
        "      x0,x1,y1,y2 = float(parts[0]), float(parts[1]), float(parts[2]), float(parts[3])\n",
        "      dataset.append((x0,x1,y1,y2))\n",
        "    return dataset\n",
        "\n",
        "dataset = read_file('ML_Colab/datasetAND+XOR.csv')\n",
        "\"\"\"it could also work with numpy and/or pandas to read the whole csv file at the\n",
        "same time, probably more efficient too\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBz_LId6fAam"
      },
      "source": [
        "We now have our dataset ready for use, next let's go and make the vectors for use with our network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF8Oo0fsfMsz"
      },
      "source": [
        "import numpy as np\n",
        "import csv"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YndtPJIvjWle"
      },
      "source": [
        "#first layer is input, meaning we take the first 2 columns of the dataset\n",
        "num_trains = len(dataset)\n",
        "X = np.ones((num_trains,1)) \n",
        "#now we add the bias to every training instance\n",
        "X = np.append(X, np.array(dataset)[:,0:2], axis=1)\n",
        "# now we take the outputs\n",
        "Y = np.array(dataset)[:,2:4]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNCs5cTggxQZ",
        "outputId": "24f4063f-bb09-4201-e8e1-214a62c30afe"
      },
      "source": [
        "#now we create Numpy arrays so that we have our weights\n",
        "#the first layer\n",
        "W1 = np.random.uniform(-1,1,(3,3))\n",
        "#the second\n",
        "W2 = np.random.uniform(-1,1,(4,2))\n",
        "print(\"W1:\\n\",W1,\"\\n\")\n",
        "print(\"W2:\\n\",W2,\"\\n\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:\n",
            " [[-0.1485472   0.34717526  0.87401992]\n",
            " [ 0.25400568 -0.67257853 -0.13334229]\n",
            " [-0.91350926 -0.13719468 -0.29667054]] \n",
            "\n",
            "W2:\n",
            " [[-0.32814207  0.35365553]\n",
            " [ 0.92442804 -0.90134961]\n",
            " [ 0.32653827  0.09339608]\n",
            " [ 0.38228571 -0.23784318]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTXmd0W6n3sY"
      },
      "source": [
        "#Now we're getting to the fun stuff\n",
        "#First, defining the necessary functions\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "#Forward Propagation\n",
        "def forward_prop():\n",
        "  #calculating the steps\n",
        "  z2 = np.matmul(X, W1)\n",
        "  a2 = sigmoid(z2)\n",
        "  a2 = np.insert(a2, 0, np.ones((num_trains,)), axis = 1)\n",
        "  z3 = np.matmul(a2, W2)\n",
        "  y_hat = sigmoid(z3)\n",
        "  return z2, a2, z3, y_hat\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCjkG9R3u74D"
      },
      "source": [
        "#Ok, so that was forward propagation\n",
        "#Now, the NN should also be able to learn, so we have to implement Backprop\n",
        "#Since we're guessing at best at the moment as to the correct outputs\n",
        "#We have to define several functions for Backprop to work\n",
        "#These include the partial derivatives with which we'll update the weights matrices W1 and W2\n",
        "#And also the backprop itself which will go over epochs or whatever we may call it\n",
        "\n",
        "# matrices sizes\n",
        "# X = num_trains x 3\n",
        "# W1 = 3 x 3\n",
        "# Z2 = num_trains x 3\n",
        "# A2 = num_trains x 4\n",
        "# W2 = 4 x 2\n",
        "# Z3 = num_trains x 2\n",
        "# A3 = Y_hat = num_trains x 2\n",
        "\n",
        "\n",
        "def sigmoid_gradient(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "def derivative(z2, a2, z3, y_hat):\n",
        "  dz3 = sigmoid_gradient(z3) # num_trains x 2\n",
        "  dw2 = np.matmul(a2.transpose(), dz3) # 4 x 2\n",
        "  dz2 = sigmoid_gradient(z2) # num_trains x 3\n",
        "  temp1 = np.matmul(dz3, W2.transpose()) # num_trains x 4\n",
        "  temp1 = np.delete(temp1, 3, 1) # drop last column because it's for bias, num_trains x 3\n",
        "  temp2 = np.multiply(temp1, dz2) # num_trains x 3\n",
        "  dw1 = np.matmul(temp2.transpose(), X) # 3 x 3\n",
        "  return dw1, dw2\n",
        "\n",
        "def backprop(epochs, learning_rate, W1, W2):\n",
        "  for i in range(epochs):\n",
        "    print(\"epoch: \", i+1, \"\\n\")\n",
        "    z2, a2, z3, y_hat = forward_prop()\n",
        "    dw1, dw2 = derivative(z2, a2, z3, y_hat)\n",
        "    W1 = W1 - learning_rate * dw1\n",
        "    W2 = W2 - learning_rate * dw2\n",
        "  with open(\"ML_Colab/AND+XORmodelW1.csv\", 'w') as fw1:\n",
        "    write = csv.writer(fw1)\n",
        "    write.writerow(W1)\n",
        "  with open(\"ML_Colab/AND+XORmodelW2.csv\", 'w') as fw2:\n",
        "    write = csv.writer(fw2)\n",
        "    write.writerow(W2)\n",
        "\n",
        "backprop(50, 0.0001, W1, W2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNi9VDPNvOb6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}